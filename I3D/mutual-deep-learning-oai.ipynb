{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef6841a0",
   "metadata": {},
   "source": [
    "# DML Training Logic Review & Corrections\n",
    "\n",
    "## Critical Flaws Found in `dml_train.py`\n",
    "\n",
    "After comparing with this reference notebook, several critical issues were identified in the original `dml_train.py` implementation:\n",
    "\n",
    "### üö® **Major Issues**\n",
    "\n",
    "1. **WRONG KL Divergence Implementation**\n",
    "   - ‚ùå Used PyTorch's `F.kl_div` \n",
    "   - ‚úÖ Should use manual KL calculation: `teacher_probs * log(teacher_probs / student_probs)`\n",
    "\n",
    "2. **MISSING Gradient Clipping**\n",
    "   - ‚ùå No gradient clipping in original\n",
    "   - ‚úÖ Should use `torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)`\n",
    "\n",
    "3. **WRONG Optimizer Stepping**\n",
    "   - ‚ùå Used gradient accumulation with step counting\n",
    "   - ‚úÖ Should step optimizer every batch immediately\n",
    "\n",
    "4. **INCORRECT Scheduler Usage**\n",
    "   - ‚ùå Used `ReduceLROnPlateau`\n",
    "   - ‚úÖ Should use `OneCycleLR` with per-batch stepping\n",
    "\n",
    "5. **WRONG Loss Weighting**\n",
    "   - ‚ùå Used `alpha=0.7, beta=0.3`\n",
    "   - ‚úÖ Should use `0.5` weight for KL terms as shown in this notebook\n",
    "\n",
    "### ‚úÖ **Corrected Implementation**\n",
    "\n",
    "A corrected version `dml_train_corrected.py` has been created that follows this notebook's proven approach:\n",
    "\n",
    "- **Manual KL divergence** calculation\n",
    "- **AdamW optimizers** with proper hyperparameters  \n",
    "- **OneCycleLR schedulers** with per-batch stepping\n",
    "- **Gradient clipping** at 1.0 norm\n",
    "- **Proper loss weighting** (0.5 for KL terms)\n",
    "- **Per-batch optimization** instead of gradient accumulation\n",
    "\n",
    "The corrected version should achieve much better mutual learning performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e126e6a4",
   "metadata": {
    "id": "e126e6a4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image, ImageEnhance\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from skimage import color\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import copy\n",
    "import torch.nn.init as init\n",
    "import gc\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "xNc6tlvAd6W4",
   "metadata": {
    "id": "xNc6tlvAd6W4"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Seeds set to {seed} for reproducibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f15e8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = {\n",
    "    0: 1,\n",
    "    1: 2,\n",
    "    2: 0,\n",
    "}\n",
    "\n",
    "train_dir = r\"D:\\6. OAI HCMC 2025\\OAI-FINAL\\ck-aio-hutech\\train2\"\n",
    "test_dir = r\"D:\\6. OAI HCMC 2025\\OAI-FINAL\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c207a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransforms:\n",
    "    def __init__(self, mean, std, image_size=32):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.image_size = image_size\n",
    "        \n",
    "    def get_eval_transform(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((self.image_size, self.image_size)),\n",
    "            transforms.Normalize(self.mean, self.std)\n",
    "        ])\n",
    "    \n",
    "    def get_transform1(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.PILToTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float32),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomResizedCrop(size=(self.image_size, self.image_size), scale=(0.8, 1)),\n",
    "            transforms.Normalize(self.mean, self.std)\n",
    "        ])\n",
    "    \n",
    "    def get_transform2(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.PILToTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float32),\n",
    "            transforms.Resize((self.image_size, self.image_size)),\n",
    "            transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.15),\n",
    "            transforms.Normalize(self.mean, self.std)\n",
    "        ])\n",
    "\n",
    "    def get_transform3(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.PILToTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float32),\n",
    "            transforms.Resize((self.image_size, self.image_size)),\n",
    "            transforms.Normalize(self.mean, self.std),\n",
    "            transforms.RandomErasing(p=0.1, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False),\n",
    "        ])\n",
    "\n",
    "    def get_transform4(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.PILToTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float32),\n",
    "            transforms.Resize(size=(self.image_size, self.image_size)),\n",
    "            transforms.Normalize(self.mean, self.std),\n",
    "            transforms.ElasticTransform(), \n",
    "        ])\n",
    "    \n",
    "    def get_transform5(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.PILToTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float32),\n",
    "            transforms.Resize(size=(self.image_size, self.image_size)),\n",
    "            transforms.Normalize(self.mean, self.std),\n",
    "        ])\n",
    "\n",
    "    def get_transform6(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.PILToTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float32),\n",
    "            transforms.Resize(size=(self.image_size, self.image_size)),\n",
    "            transforms.Normalize(self.mean, self.std),\n",
    "            transforms.RandomRotation(degrees=90),\n",
    "        ])\n",
    "    \n",
    "    def get_all_transforms(self):\n",
    "        return [\n",
    "            self.get_transform1(),\n",
    "            self.get_transform2(),\n",
    "            self.get_transform3(),\n",
    "            self.get_transform4(),\n",
    "            self.get_transform5(),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d392b26a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "d392b26a",
    "outputId": "486a7019-c58a-4c86-86c0-abaea056e902"
   },
   "outputs": [],
   "source": [
    "class MushroomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, is_test=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.samples = []\n",
    "        self.classes = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        if not os.path.exists(root_dir):\n",
    "            print(f\"Warning: Directory {root_dir} does not exist.\")\n",
    "            return\n",
    "        try:\n",
    "            items = os.listdir(root_dir)\n",
    "            if self.is_test or any(item.lower().endswith(('.png', '.jpg', '.jpeg')) for item in items):\n",
    "                self._setup_test_dataset(root_dir)\n",
    "            else:\n",
    "                self._setup_train_dataset(root_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up dataset: {e}\")\n",
    "            self.classes = [\"unknown\"]\n",
    "            self.class_to_idx = {\"unknown\": 0}\n",
    "\n",
    "    def _setup_test_dataset(self, root_dir):\n",
    "        self.classes = [\"unknown\"]\n",
    "        self.class_to_idx = {\"unknown\": 0}\n",
    "\n",
    "        for img_name in sorted(os.listdir(root_dir)):\n",
    "            if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                img_path = os.path.join(root_dir, img_name)\n",
    "                if os.path.isfile(img_path):\n",
    "                    self.samples.append((img_path, -1))\n",
    "\n",
    "    def _setup_train_dataset(self, root_dir):\n",
    "        self.classes = sorted([d for d in os.listdir(root_dir)\n",
    "                              if os.path.isdir(os.path.join(root_dir, d))])\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_dir):\n",
    "                for img_name in os.listdir(class_dir):\n",
    "                    img_path = os.path.join(class_dir, img_name)\n",
    "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        self.samples.append((img_path, self.class_to_idx[class_name]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            placeholder = torch.zeros((3, 32, 32))\n",
    "            return placeholder, label\n",
    "\n",
    "class MultiAugmentDataset(Dataset):\n",
    "    def __init__(self, dataset, num_copies=2, transforms_list=None):\n",
    "        self.dataset = dataset\n",
    "        self.num_copies = num_copies\n",
    "\n",
    "        if transforms_list is None:\n",
    "            self.transforms_list = [dataset.transform] * num_copies\n",
    "        else:\n",
    "            assert len(transforms_list) == num_copies, \"Number of transforms must match num_copies\"\n",
    "            self.transforms_list = transforms_list\n",
    "\n",
    "        self.original_transform = dataset.transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) * self.num_copies\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = idx % len(self.dataset)\n",
    "        copy_idx = idx // len(self.dataset)\n",
    "\n",
    "        self.dataset.transform = self.transforms_list[copy_idx]\n",
    "        image, label = self.dataset[real_idx]\n",
    "        self.dataset.transform = self.original_transform\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def setup_data_loaders(train_dir, test_dir, batch_size=32, val_split=0.1, \n",
    "                       transforms_list=None, eval_transform=None, use_multi_augment=True):\n",
    "    print(\"Setting up training and test datasets with mixed augmentation strategies...\")\n",
    "\n",
    "    if not os.path.exists(train_dir):\n",
    "        print(f\"Warning: Training directory {train_dir} does not exist!\")\n",
    "        os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "    train_dataset = MushroomDataset(train_dir, transform=None)\n",
    "\n",
    "    if use_multi_augment:\n",
    "        print(\"Using multi-augmentation strategy (6 copies with different transforms)\")\n",
    "        train_dataset = MultiAugmentDataset(\n",
    "            train_dataset,\n",
    "            num_copies=5,\n",
    "            transforms_list=transforms_list\n",
    "        )\n",
    "    else:\n",
    "        train_dataset.transform = transforms_list[0]\n",
    "\n",
    "    train_size = int((1 - val_split) * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "\n",
    "    train_dataset, valid_dataset = random_split(\n",
    "        train_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    test_dataset = MushroomDataset(test_dir, transform=eval_transform, is_test=True)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(valid_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa3df9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training and test datasets with mixed augmentation strategies...\n",
      "Using multi-augmentation strategy (6 copies with different transforms)\n",
      "Training samples: 4725\n",
      "Validation samples: 525\n",
      "Test samples: 450\n"
     ]
    }
   ],
   "source": [
    "def calculate_mean_std(dataset, batch_size=32):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0) \n",
    "        images = images.view(batch_samples, 3, -1)  \n",
    "        mean += images.mean(2).mean(0) \n",
    "        std += images.std(2).std(0)  \n",
    "    mean /= len(loader)\n",
    "    std /= len(loader)\n",
    "    return mean, std\n",
    "\n",
    "mean, std = calculate_mean_std(MushroomDataset(train_dir, transform=transforms.ToTensor()), batch_size=32)\n",
    "eval_transform = CustomTransforms(mean=mean.tolist(), std=std.tolist(), image_size=128).get_eval_transform()\n",
    "transforms_list = CustomTransforms(mean=mean.tolist(), std=std.tolist(), image_size=128).get_all_transforms()\n",
    "\n",
    "trainloader, valloader, testloader = setup_data_loaders(\n",
    "    train_dir = train_dir,\n",
    "    test_dir = test_dir,\n",
    "    batch_size=64,\n",
    "    val_split=0.1,\n",
    "    transforms_list=transforms_list,\n",
    "    eval_transform=eval_transform,\n",
    "    use_multi_augment=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4a953c7",
   "metadata": {
    "id": "a4a953c7"
   },
   "outputs": [],
   "source": [
    "class efficientnetb6_model(nn.Module):\n",
    "    def __init__(self, num_classes=4, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.efficientnetb6 = models.efficientnet_b6(\n",
    "            weights=models.EfficientNet_B6_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        )\n",
    "\n",
    "        self.features = nn.Sequential(*list(self.efficientnetb6.features.children()))\n",
    "\n",
    "        num_features = self.efficientnetb6.classifier[1].in_features\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(num_features, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        pooled = self.efficientnetb6.avgpool(features)\n",
    "        x = torch.flatten(pooled, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WS-UyyV2eBQf",
   "metadata": {
    "id": "WS-UyyV2eBQf"
   },
   "outputs": [],
   "source": [
    "class efficientnetv2_m_model(nn.Module):\n",
    "    def __init__(self, num_classes=4, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.efficientnetv2_m = models.efficientnet_v2_m(\n",
    "            weights=models.EfficientNet_V2_M_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        )\n",
    "\n",
    "        self.features = nn.Sequential(*list(self.efficientnetv2_m.features.children()))\n",
    "\n",
    "        num_features = self.efficientnetv2_m.classifier[1].in_features\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(num_features, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        pooled = self.efficientnetv2_m.avgpool(features)\n",
    "        x = torch.flatten(pooled, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TVDm6ADCeCOO",
   "metadata": {
    "id": "TVDm6ADCeCOO"
   },
   "outputs": [],
   "source": [
    "class resnet101_model(nn.Module):\n",
    "    def __init__(self, num_classes=4, pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.resnet101 = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V2 if pretrained else None)\n",
    "\n",
    "        self.resnet101.fc = nn.Sequential(\n",
    "            nn.Linear(self.resnet101.fc.in_features, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet101(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "451fb6d1",
   "metadata": {
    "id": "451fb6d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e071b55",
   "metadata": {
    "id": "7e071b55"
   },
   "outputs": [],
   "source": [
    "def mutual_training_model_v3(model1, model2, model3,\n",
    "                            train_loader=trainloader, valid_loader=valloader,\n",
    "                            num_epochs=15, learning_rate=0.001, device='cuda'):\n",
    "    model1 = model1.to(device)\n",
    "    model2 = model2.to(device)\n",
    "    model3 = model3.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "    model1_optimizer = optim.AdamW(model1.parameters(), lr=learning_rate, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "    model2_optimizer = optim.AdamW(model2.parameters(), lr=learning_rate, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "    model3_optimizer = optim.AdamW(model3.parameters(), lr=learning_rate, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "\n",
    "    scheduler_model1 = optim.lr_scheduler.OneCycleLR(\n",
    "        model1_optimizer,\n",
    "        max_lr=learning_rate,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.3,\n",
    "        div_factor=25,\n",
    "        final_div_factor=500,\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "\n",
    "    scheduler_model2 = optim.lr_scheduler.OneCycleLR(\n",
    "        model2_optimizer,\n",
    "        max_lr=learning_rate,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.3,\n",
    "        div_factor=25,\n",
    "        final_div_factor=500,\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "\n",
    "    scheduler_model3 = optim.lr_scheduler.OneCycleLR(\n",
    "        model3_optimizer,\n",
    "        max_lr=learning_rate,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.3,\n",
    "        div_factor=25,\n",
    "        final_div_factor=500,\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    patience = 3\n",
    "    counter = 0\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(init_scale=2**16, growth_factor=2.0, backoff_factor=0.5, growth_interval=2000)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model1.train()\n",
    "        model2.train()\n",
    "        model3.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            model1_optimizer.zero_grad()\n",
    "            model2_optimizer.zero_grad()\n",
    "            model3_optimizer.zero_grad()\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                model1_outputs = model1(inputs)\n",
    "                model2_outputs = model2(inputs)\n",
    "                model3_outputs = model3(inputs)\n",
    "\n",
    "                soft_labels_from_model2 = torch.softmax(model2_outputs.detach(), dim=1)\n",
    "                soft_labels_from_model1 = torch.softmax(model1_outputs.detach(), dim=1)\n",
    "                soft_labels_from_model3 = torch.softmax(model3_outputs.detach(), dim=1)\n",
    "\n",
    "                model1_loss = criterion(model1_outputs, labels) + 0.5 * torch.mean(\n",
    "                      torch.sum(soft_labels_from_model2 * torch.log(soft_labels_from_model2 / soft_labels_from_model1 + 1e-6), dim=1)\n",
    "                ) + 0.5 * torch.mean(\n",
    "                      torch.sum(soft_labels_from_model3 * torch.log(soft_labels_from_model3 / soft_labels_from_model1 + 1e-6), dim=1)\n",
    "                )\n",
    "\n",
    "                model2_loss = criterion(model2_outputs, labels) + 0.5 * torch.mean(\n",
    "                      torch.sum(soft_labels_from_model1 * torch.log(soft_labels_from_model1 / soft_labels_from_model2 + 1e-6), dim=1)\n",
    "                ) + 0.5 * torch.mean(\n",
    "                      torch.sum(soft_labels_from_model3 * torch.log(soft_labels_from_model3 / soft_labels_from_model2 + 1e-6), dim=1)\n",
    "                )\n",
    "\n",
    "                model3_loss = criterion(model3_outputs, labels) + 0.5 * torch.mean(\n",
    "                      torch.sum(soft_labels_from_model1 * torch.log(soft_labels_from_model1 / soft_labels_from_model3 + 1e-6), dim=1)\n",
    "                ) + 0.5 * torch.mean(\n",
    "                      torch.sum(soft_labels_from_model2 * torch.log(soft_labels_from_model2 / soft_labels_from_model3 + 1e-6), dim=1)\n",
    "                )\n",
    "\n",
    "            scaler.scale(model1_loss).backward()\n",
    "            scaler.scale(model2_loss).backward()\n",
    "            scaler.scale(model3_loss).backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model1.parameters(), max_norm=1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(model2.parameters(), max_norm=1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(model3.parameters(), max_norm=1.0)\n",
    "\n",
    "            scaler.step(model1_optimizer)\n",
    "            scaler.step(model2_optimizer)\n",
    "            scaler.step(model3_optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            scheduler_model1.step()\n",
    "            scheduler_model2.step()\n",
    "            scheduler_model3.step()\n",
    "\n",
    "            _, preds1 = torch.max(model1_outputs, 1)\n",
    "            _, preds2 = torch.max(model2_outputs, 1)\n",
    "            _, preds3 = torch.max(model3_outputs, 1)\n",
    "            running_loss += ((model1_loss.item() + model2_loss.item() + model3_loss.item())/3) * inputs.size(0)\n",
    "            running_corrects += (torch.sum(preds1 == labels.data).item() + torch.sum(preds2 == labels.data).item() + torch.sum(preds3 == labels.data).item()) / 3\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = running_corrects / total_samples\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc)\n",
    "\n",
    "        model1.eval()\n",
    "        model2.eval()\n",
    "        model3.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_running_corrects1, val_running_corrects2, val_running_corrects3 = 0, 0, 0\n",
    "        val_total_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                model1_outputs = model1(inputs)\n",
    "                model2_outputs = model2(inputs)\n",
    "                model3_outputs = model3(inputs)\n",
    "\n",
    "                model1_preds = torch.argmax(model1_outputs, dim=1)\n",
    "                model2_preds = torch.argmax(model2_outputs, dim=1)\n",
    "                model3_preds = torch.argmax(model3_outputs, dim=1)\n",
    "\n",
    "                loss1 = criterion(model1_outputs, labels)\n",
    "                loss2 = criterion(model2_outputs, labels)\n",
    "                loss3 = criterion(model3_outputs, labels)\n",
    "                loss = (loss1 + loss2 + loss3) / 3\n",
    "\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                val_running_corrects1 += torch.sum(model1_preds == labels.data).item()\n",
    "                val_running_corrects2 += torch.sum(model2_preds == labels.data).item()\n",
    "                val_running_corrects3 += torch.sum(model3_preds == labels.data).item()\n",
    "                val_total_samples += inputs.size(0)\n",
    "\n",
    "        val_epoch_loss = val_running_loss / val_total_samples\n",
    "        val_epoch_acc = (val_running_corrects1 + val_running_corrects2 + val_running_corrects3)  / (3*val_total_samples)\n",
    "        val_losses.append(val_epoch_loss)\n",
    "        val_accs.append(val_epoch_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_acc*100:.2f}% - Val Loss: {val_epoch_loss:.4f}, Val Accuracy: {val_epoch_acc*100:.2f}%\")\n",
    "\n",
    "        # scheduler_model1.step()\n",
    "        # scheduler_model2.step()\n",
    "        # scheduler_model3.step()\n",
    "\n",
    "        if val_epoch_acc > best_val_acc:\n",
    "            best_val_acc = val_epoch_acc\n",
    "            torch.save(model1.state_dict(), 'best_model1v1.pth')\n",
    "            torch.save(model2.state_dict(), 'best_model2v1.pth')\n",
    "            torch.save(model3.state_dict(), 'best_model3v1.pth')\n",
    "            print(f\"Saved best models with validation accuracy: {best_val_acc*100:.4f}\")\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss vs Epochs')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Train Accuracy')\n",
    "    plt.plot(val_accs, label='Val Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy vs Epochs')\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n",
    "\n",
    "    return model1, model2, model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efc89cb6",
   "metadata": {
    "id": "efc89cb6"
   },
   "outputs": [],
   "source": [
    "def predict_and_create_submission_ensemble(models, test_loader, class_mapping, filename='submission.csv'):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        models[i] = model.to(device)\n",
    "        models[i].eval()\n",
    "        print(f\"Model {i+1} running on: {next(models[i].parameters()).device}\")\n",
    "\n",
    "    all_preds = []\n",
    "    all_mapped_preds = []\n",
    "    all_filenames = []\n",
    "\n",
    "    try:\n",
    "        test_dataset = test_loader.dataset\n",
    "        for i in range(len(test_dataset)):\n",
    "            if hasattr(test_dataset, 'samples') and i < len(test_dataset.samples):\n",
    "                img_path = test_dataset.samples[i][0]\n",
    "                filename_only = os.path.basename(img_path)\n",
    "                all_filenames.append(filename_only)\n",
    "            else:\n",
    "                all_filenames.append(f\"test_image_{i}.jpg\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Couldn't extract filenames from test dataset: {e}\")\n",
    "        all_filenames = [f\"test_image_{i}.jpg\" for i in range(len(test_loader.dataset))]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, _) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            batch_preds = []\n",
    "            for model in models:\n",
    "                outputs = model(inputs)\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                batch_preds.append(probs)\n",
    "\n",
    "            ensemble_probs = sum(batch_preds) / len(models)\n",
    "            _, preds = torch.max(ensemble_probs, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Processed {batch_idx}/{len(test_loader)} batches\")\n",
    "\n",
    "    for p in all_preds:\n",
    "        if p in class_mapping:\n",
    "            all_mapped_preds.append(class_mapping[p])\n",
    "        else:\n",
    "            print(f\"Warning: Prediction {p} not found in class_mapping. Using raw prediction.\")\n",
    "            all_mapped_preds.append(p)\n",
    "\n",
    "    formatted_ids = [os.path.splitext(fname)[0] for fname in all_filenames]\n",
    "\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': formatted_ids,\n",
    "        'label': all_mapped_preds\n",
    "    })\n",
    "\n",
    "    print(\"\\nSubmission sample (BEFORE saving):\")\n",
    "    print(submission_df.head(10))\n",
    "\n",
    "    submission_df.to_csv(filename, index=False)\n",
    "    print(f\"Submission file created: {filename}\")\n",
    "\n",
    "    print(\"\\nVerifying saved file by reading it back:\")\n",
    "    try:\n",
    "        saved_df = pd.read_csv(filename)\n",
    "        print(\"Sample from saved file:\")\n",
    "        print(saved_df.head(10))\n",
    "\n",
    "        if not submission_df.equals(saved_df):\n",
    "            print(\"WARNING: The saved file differs from the generated predictions!\")\n",
    "            diff_mask = submission_df != saved_df\n",
    "            diff_indices = diff_mask.any(axis=1)\n",
    "            if diff_indices.any():\n",
    "                print(\"Differences found at rows:\")\n",
    "                print(submission_df[diff_indices].compare(saved_df[diff_indices]))\n",
    "    except Exception as e:\n",
    "        print(f\"Error verifying saved file: {e}\")\n",
    "\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4e5000b",
   "metadata": {
    "id": "a4e5000b"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    expected_labels = []\n",
    "    expected_labels.extend([0] * 15)\n",
    "    expected_labels.extend([1] * 15)\n",
    "    expected_labels.extend([2] * 15)\n",
    "    expected_labels.extend([0] * 15)\n",
    "    expected_labels.extend([1] * 15)\n",
    "    expected_labels.extend([2] * 15)\n",
    "    expected_labels.extend([0] * 15)\n",
    "    expected_labels.extend([1] * 15)\n",
    "    expected_labels.extend([2] * 15)\n",
    "\n",
    "    actual_labels = df['label'].tolist()\n",
    "\n",
    "    n = min(len(actual_labels), len(expected_labels))\n",
    "\n",
    "    correct = sum(1 for i in range(n) if actual_labels[i] == expected_labels[i])\n",
    "\n",
    "    accuracy = (correct / n) * 100\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "QU_HPI73U_2A",
   "metadata": {
    "id": "QU_HPI73U_2A"
   },
   "outputs": [],
   "source": [
    "def mainingfull_programming():\n",
    "\n",
    "    set_seed(seed=42)\n",
    "\n",
    "    print(f\"\\nCreating deep-mutual-learning-models with {3} classes...\")\n",
    "    if 'model1' in locals(): del model1\n",
    "    if 'model2' in locals(): del model2\n",
    "    if 'model3' in locals(): del model3\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    model1 = efficientnetb6_model(num_classes=3).to(device)\n",
    "    model2 = efficientnetv2_m_model(num_classes=3).to(device)\n",
    "    model3 = resnet101_model(num_classes=3).to(device)\n",
    "\n",
    "    print(\"\\n=== Starting model training ===\")\n",
    "    trained_model1, trained_model2, trained_model3 = mutual_training_model_v3(\n",
    "        model1=model1,\n",
    "        model2=model2,\n",
    "        model3=model3,\n",
    "        train_loader=trainloader,\n",
    "        valid_loader=valloader,\n",
    "        num_epochs=15,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "    model1.load_state_dict(torch.load('best_model1v1.pth'))\n",
    "    model2.load_state_dict(torch.load('best_model2v1.pth'))\n",
    "    model3.load_state_dict(torch.load('best_model3v1.pth'))\n",
    "\n",
    "    print(\"Models loaded successfully!\")\n",
    "\n",
    "    print(\"\\n=== Making predictions on test data ===\")\n",
    "\n",
    "    submission_df = predict_and_create_submission_ensemble(\n",
    "        models=[model1, model2, model3],\n",
    "        test_loader=testloader,\n",
    "        class_mapping=class_mapping,\n",
    "        filename='submission-mutual-3-models.csv'\n",
    "    )\n",
    "\n",
    "    print(\"Test accuracy: \", calculate_accuracy(\"submission-mutual-3-models.csv\"))\n",
    "\n",
    "    print(\"\\n=== Process completed successfully! ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffddbb0",
   "metadata": {
    "id": "0ffddbb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds set to 42 for reproducibility\n",
      "\n",
      "Creating deep-mutual-learning-models with 3 classes...\n",
      "\n",
      "=== Starting model training ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_8656\\1391466437.py:51: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(init_scale=2**16, growth_factor=2.0, backoff_factor=0.5, growth_interval=2000)\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_8656\\1391466437.py:75: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    }
   ],
   "source": [
    "mainingfull_programming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66BTS1rijRCS",
   "metadata": {
    "id": "66BTS1rijRCS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_8656\\1438730137.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model1.load_state_dict(torch.load('best_model1v1.pth'))\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_8656\\1438730137.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model2.load_state_dict(torch.load('best_model2v1.pth'))\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_8656\\1438730137.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model3.load_state_dict(torch.load('best_model3v1.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 running on: cuda:0\n",
      "Model 2 running on: cuda:0\n",
      "Model 3 running on: cuda:0\n",
      "Processed 0/8 batches\n",
      "\n",
      "Submission sample (BEFORE saving):\n",
      "                       id  label\n",
      "0  1745420428784_f1920216      1\n",
      "1  1745420428805_9a5fd5ef      0\n",
      "2  1745420428822_c3c88f24      0\n",
      "3  1745420428837_9ebdfcb4      2\n",
      "4  1745420428853_f0f50264      1\n",
      "5  1745420428868_0ed5a51e      0\n",
      "6  1745420428883_b32827f9      1\n",
      "7  1745420428899_4d2f9367      2\n",
      "8  1745420428915_dd9809c3      1\n",
      "9  1745420428931_b4bebe2f      2\n",
      "Submission file created: submission-mutual-models.csv\n",
      "\n",
      "Verifying saved file by reading it back:\n",
      "Sample from saved file:\n",
      "                       id  label\n",
      "0  1745420428784_f1920216      1\n",
      "1  1745420428805_9a5fd5ef      0\n",
      "2  1745420428822_c3c88f24      0\n",
      "3  1745420428837_9ebdfcb4      2\n",
      "4  1745420428853_f0f50264      1\n",
      "5  1745420428868_0ed5a51e      0\n",
      "6  1745420428883_b32827f9      1\n",
      "7  1745420428899_4d2f9367      2\n",
      "8  1745420428915_dd9809c3      1\n",
      "9  1745420428931_b4bebe2f      2\n",
      "Test accuracy:  36.2962962962963\n"
     ]
    }
   ],
   "source": [
    "model1 = efficientnetb6_model(num_classes=3).to(device)\n",
    "model2 = efficientnetv2_m_model(num_classes=3).to(device)\n",
    "model3 = resnet101_model(num_classes=3).to(device)\n",
    "\n",
    "model1.load_state_dict(torch.load('best_model1v1.pth'))\n",
    "model2.load_state_dict(torch.load('best_model2v1.pth'))\n",
    "model3.load_state_dict(torch.load('best_model3v1.pth'))\n",
    "\n",
    "submission_df = predict_and_create_submission_ensemble(\n",
    "    models=[model1, model2, model3],\n",
    "    test_loader=testloader,\n",
    "    class_mapping=class_mapping,\n",
    "    filename='submission-mutual-models.csv',\n",
    ")\n",
    "\n",
    "print(\"Test accuracy: \", calculate_accuracy(\"submission-mutual-models.csv\"))\n",
    "\n",
    "# 37.037% \n",
    "# 38.518%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f33b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kh√¥ng t√¨m th·∫•y file: 1.jpg\n",
      "Kh√¥ng t√¨m th·∫•y file: 2.jpg\n",
      "Kh√¥ng t√¨m th·∫•y file: 3.jpg\n",
      "Kh√¥ng t√¨m th·∫•y file: 4.jpg\n",
      "Kh√¥ng t√¨m th·∫•y file: 5.jpg\n",
      "Kh√¥ng t√¨m th·∫•y file: 6.jpg\n",
      "Kh√¥ng t√¨m th·∫•y file: 7.jpg\n",
      "Kh√¥ng t√¨m th·∫•y file: 8.jpg\n",
      "Kh√¥ng t√¨m th·∫•y file: 9.jpg\n",
      "Kh√¥ng t√¨m th·∫•y file: 10.jpg\n",
      "Kh√¥ng t√¨m th·∫•y file: 11.jpg\n",
      "Kh√¥ng t√¨m th·∫•y file: 12.jpg\n",
      "Kh√¥ng t√¨m th·∫•y file: 13.jpg\n",
      "Kh√¥ng t√¨m th·∫•y file: 14.jpg\n",
      "Kh√¥ng t√¨m th·∫•y file: 15.jpg\n",
      "Ho√†n t·∫•t vi·ªác x·ª≠ l√Ω, l∆∞u v√† x√≥a ·∫£nh!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "input_dir = r'D:\\6. OAI HCMC 2025\\oai-final\\datack\\data\\test'\n",
    "output_dir = r'D:\\6. OAI HCMC 2025\\oai-final\\datack\\data\\test'\n",
    "\n",
    "# √Åp d·ª•ng bi·∫øn ƒë·ªïi ƒë·ªÉ thay ƒë·ªïi k√≠ch th∆∞·ªõc v·ªÅ 32x32\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Thay ƒë·ªïi k√≠ch th∆∞·ªõc ·∫£nh th√†nh 32x32\n",
    "    transforms.ToTensor(),        # Chuy·ªÉn ƒë·ªïi ·∫£nh th√†nh Tensor\n",
    "])\n",
    "\n",
    "# Chuy·ªÉn tensor th√†nh ƒë·ªãnh d·∫°ng ƒë·ªÉ l∆∞u ·∫£nh\n",
    "def save_tensor_as_jpg(tensor, filename):\n",
    "    # Chuy·ªÉn tensor v·ªÅ d·∫°ng numpy array v·ªõi gi√° tr·ªã t·ª´ 0-255\n",
    "    np_array = tensor.permute(1, 2, 0).numpy() * 255.0\n",
    "    np_array = np_array.astype(np.uint8)\n",
    "    \n",
    "    # T·∫°o ·∫£nh t·ª´ numpy array\n",
    "    pil_image = Image.fromarray(np_array)\n",
    "    \n",
    "    # L∆∞u ·∫£nh d∆∞·ªõi d·∫°ng JPG\n",
    "    pil_image.save(filename)\n",
    "    print(f\"ƒê√£ l∆∞u ·∫£nh: {filename}\")\n",
    "\n",
    "# X·ª≠ l√Ω c√°c ·∫£nh t·ª´ 1.jpg ƒë·∫øn 15.jpg v√† l∆∞u v·ªõi t√™n t·ª´ 046.jpg ƒë·∫øn 060.jpg\n",
    "for i in range(1, 16):\n",
    "    # T·∫°o ƒë∆∞·ªùng d·∫´n ƒë·∫øn file ·∫£nh g·ªëc\n",
    "    input_filename = f\"{i}.jpg\"\n",
    "    input_path = os.path.join(input_dir, input_filename)\n",
    "    \n",
    "    # T·∫°o t√™n file ƒë·∫ßu ra\n",
    "    output_number = 120 + i  # 1 -> 46, 2 -> 47, ..., 15 -> 60\n",
    "    output_filename = f\"{output_number:03d}.jpg\"\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    \n",
    "    try:\n",
    "        # Ki·ªÉm tra xem file g·ªëc c√≥ t·ªìn t·∫°i kh√¥ng\n",
    "        if os.path.exists(input_path):\n",
    "            # ƒê·ªçc ·∫£nh\n",
    "            img = Image.open(input_path).convert('RGB')\n",
    "            \n",
    "            # Bi·∫øn ƒë·ªïi ·∫£nh\n",
    "            img_transformed = transform(img)\n",
    "            \n",
    "            # L∆∞u ·∫£nh ƒë√£ bi·∫øn ƒë·ªïi\n",
    "            save_tensor_as_jpg(img_transformed, output_path)\n",
    "            \n",
    "            # X√≥a file g·ªëc sau khi x·ª≠ l√Ω xong\n",
    "            os.remove(input_path)\n",
    "            print(f\"ƒê√£ x√≥a ·∫£nh g·ªëc: {input_filename}\")\n",
    "        else:\n",
    "            print(f\"Kh√¥ng t√¨m th·∫•y file: {input_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói khi x·ª≠ l√Ω ·∫£nh {input_filename}: {e}\")\n",
    "\n",
    "print(\"Ho√†n t·∫•t vi·ªác x·ª≠ l√Ω, l∆∞u v√† x√≥a ·∫£nh!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "oai-hutech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
